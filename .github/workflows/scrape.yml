name: Web Scraper Workflow

on:
  schedule:
    - cron: '0 0 * * *'  # Run daily at midnight
  workflow_dispatch:  # Allow manual trigger

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Setup Chrome
        id: setup-chrome
        uses: browser-actions/setup-chrome@v1
        with:
          chrome-version: stable
          install-chromedriver: true
          install-dependencies: true

      - name: Install Node dependencies
        run: |
          npm install puppeteer axios fs-extra google-spreadsheet google-auth-library

      - name: Run scraper
        run: node templates/scraper.js
        env:
          CHROME_PATH: ${{ steps.setup-chrome.outputs.chrome-path }}
          CHROMEDRIVER_PATH: ${{ steps.setup-chrome.outputs.chromedriver-path }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          GOOGLE_DOCS_SERVICE_ACCOUNT: ${{ secrets.GOOGLE_DOCS_SERVICE_ACCOUNT }}

      - name: Upload all data artifacts
        uses: actions/upload-artifact@v4
        with:
          name: scraped-data
          path: data/
          retention-days: 7
          
      # Upload only the final Gemini output as a separate artifact for easier access
      - name: Upload Gemini output
        uses: actions/upload-artifact@v4
        with:
          name: gemini-output
          path: data/gemini_processed_data.json
          retention-days: 7
          if-no-files-found: warn
