name: Web Scraper Workflow

on:
  workflow_dispatch:  # Allow manual trigger

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Setup Chrome
        id: setup-chrome
        uses: browser-actions/setup-chrome@v1
        with:
          chrome-version: stable

      - name: Install Node dependencies
        run: |
          npm install puppeteer axios fs-extra google-spreadsheet google-auth-library

      - name: Run scraper
        run: node templates/scraper.js
        env:
          CHROME_PATH: ${{ steps.setup-chrome.outputs.chrome-path }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          GOOGLE_DOCS_SERVICE_ACCOUNT: ${{ secrets.GOOGLE_DOCS_SERVICE_ACCOUNT }}

      # This step will run regardless of the scraper's outcome.
      # It ensures all generated files (HTML, CSVs, logs, and error.json on failure) are saved.
      - name: Upload all data artifacts
        if: always() # <-- This is the key change
        uses: actions/upload-artifact@v4
        with:
          name: scraped-data
          path: data/
          retention-days: 7
          
      # Upload only the final Gemini output as a separate artifact for easier access
      # This will be skipped on failure because the file won't exist, which is the correct behavior.
      - name: Upload Gemini output
        uses: actions/upload-artifact@v4
        with:
          name: gemini-output
          path: data/gemini_processed_data.json
          retention-days: 7
          if-no-files-found: warn